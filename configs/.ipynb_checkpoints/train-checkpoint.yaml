# configs/train.yaml

defaults:
  - logger: wandb
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  output_subdir: null
  run:
    dir: ./

resume_from_checkpoint: null

pl_trainer:
  accelerator: gpu
  devices: -1
  num_nodes: 1
  precision: "bf16-mixed"
  strategy: ddp
  benchmark: true
  deterministic: false
  max_epochs: ${training.epochs}
  log_every_n_steps: 10
  num_sanity_val_steps: 0
  enable_model_summary: true
  check_val_every_n_epoch: 1

logger:
  _target_: lightning.pytorch.loggers.WandbLogger
  project: llm-training
  entity: my-lab
  save_dir: "./logs"
  settings:
    _target_: wandb.Settings
    _disable_stats: true
    _disable_meta: true
  offline: false

models:
  lm:
    _target_: transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: "gpt2"
    trust_remote_code: false

tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: "gpt2"
  padding_side: "right"
  truncation: true
  use_fast: true

optimizer:
  _target_: torch.optim.AdamW
  lr: ${training.learning_rate}
  weight_decay: ${training.weight_decay}

scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: ${training.warmup_steps}
  num_training_steps: ${training.epochs}

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"
  text_column: "text"
  max_seq_length: 512
  batch_size: ${training.batch_size}
  num_workers: 4

training:
  batch_size: 16
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 1000
  epochs: 3
  gradient_clip_val: 1.0
  grad_accum_steps: 2
  ema_decay: -1 # disable by default, enable with >0

inference:
  max_new_tokens: 128
  temperature: 1.0
  top_k: 50
  top_p: 0.95
