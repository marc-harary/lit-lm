seed_everything: 0

trainer:
  accelerator: gpu
  devices: -1
  num_nodes: 1
  precision: bf16-mixed
  strategy: auto
  benchmark: true
  deterministic: false
  max_epochs: 1 
  log_every_n_steps: 10
  num_sanity_val_steps: 0
  enable_model_summary: true
  check_val_every_n_epoch: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2 
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      project: bema 
      entity: pyle-lab
      settings:
        code_dir: ./lit_lm

# HuggingFace model
model:
  pretrained_model_name_or_path: gpt2

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 3e-5
    weight_decay: 0.01
    betas: [0.9, 0.999]

    # lr_scheduler:
    #   class_path: transformers.get_linear_schedule_with_warmup
    #   init_args:
    #     num_warmup_steps: ${training.warmup_steps}
    #     num_training_steps: ${training.epochs}

data:
  batch_size: 8 
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1
  pretrained_model_name_or_path: ${model.pretrained_model_name_or_path}
  text_key: text
  max_length: 512
  num_workers: 4
